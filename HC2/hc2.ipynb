{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d38c34-b9ba-4662-9e0c-5c785781d837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Did not use this model -- did not produce promising results. Scrapped pretty early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dcea450-1d70-41b3-9c36-74d55787a3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"05/03/2024 01:32:11.500110\",\"severity\":\"INFO\",\"msg\":\"Start gcsfuse/1.4.0 (Go version go1.21.5) for app \\\"\\\" using mount point: /home/jupyter/bucket\\n\"}\n"
     ]
    }
   ],
   "source": [
    "! gcsfuse --implicit-dirs --rename-dir-limit=100  --max-conns-per-host=100 hms_applied_cv \"bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b655a7-1ccb-4a75-9048-fe0e813527fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from aeon.classification.hybrid import HIVECOTEV2\n",
    "from sklearn.preprocessing import \n",
    "from sklearn.metrics import log_loss\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a051c4a-2e1a-4e18-8f74-1f9bd6e2b2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_parquet_sample(file_path, n_samples=1):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    return df.sample(n=n_samples)\n",
    "\n",
    "def load_eeg_sample(row):\n",
    "    eeg_file_path = f'bucket/hms-harmful-brain-activity-classification/train_eegs/{row.eeg_id}.parquet'\n",
    "    return load_parquet_sample(eeg_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5ca337-b01f-4928-89b5-a1ec69f98a05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Metadata loaded.\n",
      "Starting to load EEG samples in parallel...\n",
      "Progress: 100/6447 samples loaded\n",
      "Progress: 200/6447 samples loaded\n",
      "Progress: 300/6447 samples loaded\n",
      "Progress: 400/6447 samples loaded\n",
      "Progress: 500/6447 samples loaded\n",
      "Progress: 600/6447 samples loaded\n",
      "Progress: 700/6447 samples loaded\n",
      "Progress: 800/6447 samples loaded\n",
      "Progress: 900/6447 samples loaded\n",
      "Progress: 1000/6447 samples loaded\n",
      "Progress: 1100/6447 samples loaded\n",
      "Progress: 1200/6447 samples loaded\n",
      "Progress: 1300/6447 samples loaded\n",
      "Progress: 1400/6447 samples loaded\n",
      "Progress: 1500/6447 samples loaded\n",
      "Progress: 1600/6447 samples loaded\n",
      "Progress: 1700/6447 samples loaded\n",
      "Progress: 1800/6447 samples loaded\n",
      "Progress: 1900/6447 samples loaded\n",
      "Progress: 2000/6447 samples loaded\n",
      "Progress: 2100/6447 samples loaded\n",
      "Progress: 2200/6447 samples loaded\n",
      "Progress: 2300/6447 samples loaded\n",
      "Progress: 2400/6447 samples loaded\n",
      "Progress: 2500/6447 samples loaded\n",
      "Progress: 2600/6447 samples loaded\n",
      "Progress: 2700/6447 samples loaded\n",
      "Progress: 2800/6447 samples loaded\n",
      "Progress: 2900/6447 samples loaded\n",
      "Progress: 3000/6447 samples loaded\n",
      "Progress: 3100/6447 samples loaded\n",
      "Progress: 3200/6447 samples loaded\n",
      "Progress: 3300/6447 samples loaded\n",
      "Progress: 3400/6447 samples loaded\n",
      "Progress: 3500/6447 samples loaded\n",
      "Progress: 3600/6447 samples loaded\n",
      "Progress: 3700/6447 samples loaded\n",
      "Progress: 3800/6447 samples loaded\n",
      "Progress: 3900/6447 samples loaded\n",
      "Progress: 4000/6447 samples loaded\n",
      "Progress: 4100/6447 samples loaded\n",
      "Progress: 4200/6447 samples loaded\n",
      "Progress: 4300/6447 samples loaded\n",
      "Progress: 4400/6447 samples loaded\n",
      "Progress: 4500/6447 samples loaded\n",
      "Progress: 4600/6447 samples loaded\n",
      "Progress: 4700/6447 samples loaded\n",
      "Progress: 4800/6447 samples loaded\n",
      "Progress: 4900/6447 samples loaded\n",
      "Progress: 5000/6447 samples loaded\n",
      "Progress: 5100/6447 samples loaded\n",
      "Progress: 5200/6447 samples loaded\n",
      "Progress: 5300/6447 samples loaded\n",
      "Progress: 5400/6447 samples loaded\n",
      "Progress: 5500/6447 samples loaded\n",
      "Progress: 5600/6447 samples loaded\n",
      "Progress: 5700/6447 samples loaded\n",
      "Progress: 5800/6447 samples loaded\n",
      "Progress: 5900/6447 samples loaded\n",
      "Progress: 6000/6447 samples loaded\n",
      "Progress: 6100/6447 samples loaded\n",
      "Progress: 6200/6447 samples loaded\n",
      "Progress: 6300/6447 samples loaded\n",
      "Progress: 6400/6447 samples loaded\n",
      "(6435, 20)\n",
      "Starting model training...\n",
      "Model training completed in 1150.76 seconds.\n",
      "Accuracy: 0.35120435120435123\n",
      "loss: 11.997697193861745\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "print('Loading metadata...')\n",
    "metadata = pd.read_csv('bucket/hms-harmful-brain-activity-classification/cleaned_train.csv')\n",
    "\n",
    "metadata_filtered = metadata[metadata['is_center'] == True]\n",
    "\n",
    "metadata_sample = metadata_filtered.sample(frac=.3, random_state=2)\n",
    "print('Metadata loaded.')\n",
    "\n",
    "# eeg_data_samples = []\n",
    "# spectrogram_samples = []\n",
    "# total_samples = len(metadata_sample)\n",
    "# processed_samples = 0  \n",
    "\n",
    "# print(\"Starting to load EEG samples...\")\n",
    "# for idx, row in metadata_sample.iterrows():\n",
    "#     processed_samples += 1\n",
    "#     if idx % 100 == 0:  # Print progress every 10 samples\n",
    "#         print(f\"Progress: {processed_samples}/{total_samples} samples loaded\")\n",
    "#     eeg_sample = load_parquet_sample(f'bucket/hms-harmful-brain-activity-classification/train_eegs/{row[\"eeg_id\"]}.parquet')\n",
    "#     eeg_data_samples.append(eeg_sample)\n",
    "#     # spectrogram_sample = load_parquet_sample(f'bucket/hms-harmful-brain-activity-classification/train_spectrograms/{row[\"spectrogram_id\"]}.parquet')\n",
    "#     # spectrogram_samples.append(spectrogram_sample)\n",
    "\n",
    "eeg_data_samples = []\n",
    "total_samples = len(metadata_sample)\n",
    "print(\"Starting to load EEG samples in parallel...\")\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    # submit tasks to load EEG samples in parallel\n",
    "    futures = [executor.submit(load_eeg_sample, row) for index, row in metadata_sample.iterrows()]\n",
    "\n",
    "    # collect results as they are completed\n",
    "    for i, future in enumerate(futures):\n",
    "        eeg_data_samples.append(future.result())\n",
    "        if (i + 1) % 100 == 0:  # Print progress every 10 samples\n",
    "            print(f\"Progress: {i + 1}/{total_samples} samples loaded\")\n",
    "\n",
    "            \n",
    "eeg_data = pd.concat(eeg_data_samples, ignore_index=True)\n",
    "\n",
    "# drop any missing values\n",
    "indices_kept = eeg_data.dropna().index\n",
    "eeg_data = eeg_data.dropna()\n",
    "metadata_sample_filtered = metadata_sample.iloc[indices_kept]\n",
    "\n",
    "# spectrogram_data = pd.concat(spectrogram_samples, ignore_index=True)\n",
    "print(eeg_data.shape)\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(eeg_data, metadata_sample_filtered['expert_consensus'], test_size=0.2, random_state=42)\n",
    "\n",
    "# train model\n",
    "print(\"Starting model training...\")\n",
    "start_time = time.time()\n",
    "model = HIVECOTEV2(time_limit_in_minutes=0.2)\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Model training completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# save model, to use model: loaded_model = joblib.load(model_filename)\n",
    "model_filename = 'hivecotev2_model.joblib'\n",
    "joblib.dump(model, model_filename)\n",
    "\n",
    "# predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test) # get probability distribution\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_test_one_hot = lb.fit_transform(y_test)\n",
    "loss = log_loss(y_test_one_hot, y_pred_proba)\n",
    "print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fcb4b90-bcf3-45b9-9b5f-a0bdab92f954",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submissionn shape: (1, 7)\n"
     ]
    }
   ],
   "source": [
    "def predict_eeg(model, file_path):\n",
    "    eeg_data = load_parquet_sample(file_path)\n",
    "    predictions = model.predict_proba(eeg_data)\n",
    "    return predictions.flatten()\n",
    "\n",
    "loaded_model = joblib.load('hivecotev2_model.joblib')\n",
    "test_eegs_dir = Path('bucket/hms-harmful-brain-activity-classification/test_eegs')\n",
    "predictions_list = []\n",
    "for file in test_eegs_dir.iterdir():\n",
    "    if file.suffix == '.parquet':\n",
    "        # get eeg id file name\n",
    "        eeg_id = file.stem\n",
    "        \n",
    "        # Predict the probabilities for the current EEG file\n",
    "        probabilities = predict_eeg(loaded_model, file)\n",
    "        \n",
    "        # Format the prediction record and append to the list\n",
    "        predictions_list.append([eeg_id] + list(probabilities))\n",
    "        \n",
    "predictions_df = pd.DataFrame(predictions_list, columns=['eeg_id', 'seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote'])\n",
    "predictions_df.to_csv('submission.csv', index=False)\n",
    "print(f'Submissionn shape: {predictions_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f6e19fc1-c643-4a39-9b8e-d4afdb3db933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.19952867 0.         0.         0.53939523 0.26107611]]\n"
     ]
    }
   ],
   "source": [
    "eeg_test = load_parquet_sample(f'bucket/hms-harmful-brain-activity-classification/test_eegs/3911565283.parquet') # our submission is only going to be one line?\n",
    "y_test_proba = model.predict_proba(eeg_test)\n",
    "print(y_test_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f0ba3-45f6-4555-a770-fa41423bc2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0a69848-67fc-4ca1-8851-2bf633d72d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn version: 1.3.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d61c5b5-e83b-412a-9848-080b40e25ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.3.2\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting numpy<2.0,>=1.17.3 (from scikit-learn==1.3.2)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.5.0 (from scikit-learn==1.3.2)\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn==1.3.2)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.3.2)\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Saved ./scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Saved ./joblib-1.3.2-py3-none-any.whl\n",
      "Saved ./numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Saved ./scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Saved ./threadpoolctl-3.3.0-py3-none-any.whl\n",
      "Successfully downloaded scikit-learn joblib numpy scipy threadpoolctl\n"
     ]
    }
   ],
   "source": [
    "!pip download scikit-learn==1.3.2 -d \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54adb470-c357-4b00-84a9-796472575c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use in kaggle for scikit-learn version\n",
    "!pip install -U scikit-learn==1.3.2\n",
    "!pip freeze | grep scikit-learn"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
